# Assignment 9

## Objective
Implement the following metrics (either on separate models or same, your choice):
* Recall, Precision, and F1 Score
* BLEU 
* Perplexity (explain whether you are using bigram, trigram, or something else, what does your PPL score represent?)
* BERTScore (here are [1](https://colab.research.google.com/drive/1kpL8Y_AnUUiCxFjhxSrxCsc6-sDMNb_Q), [2](https://huggingface.co/metrics/bertscore) examples)

Once done, proceed to answer questions in the Assignment-Submission Page. 
Questions asked are:
* Share the link to the readme file where you have explained all 4 metrics. 
* Share the link(s) where we can find the code and training logs for all of your 4 metrics
* Share the last 2-3 epochs/stage logs for all of your 4 metrics separately (A, B, C, D) and describe your understanding about the numbers you're seeing, are they good/bad? Why?


## Evaluation Metrics

Evalauation of machine learning model is one of the major part in building a machine learning model. 'Accuracy' is one of the most commonly used Evaluation metric in the classification problems, that is the total number of correct predictions by the total number of predictions.
Accuracy, Recall, Precision, and F1- Score are the key classification metrics.


### Accuracy:

It is defined as percentage of total number of correct predictions to the total number of observations in the dataset. It can be easily calculated as total numer of correct predictions divided by total number of predictions.
<p align="center">
  <img alt="Accuracy" src="https://user-images.githubusercontent.com/36162708/124904184-5e34b100-e002-11eb-9649-debd7a008121.png">
</p>
Where,<br>
TP => True Positive<br>
TN => True Negative<br>
FP => False Positive<br>
FN => False Negative<br>


### Recall:
Recall is the percentage of relevant results that are correctly classified by the model, ie,it is the ratio of samples which were predicted to belong to a class with respect to all of the samples that truly belong in the class (predicted results).
<p align="center">
  <img alt="Recall" src="https://user-images.githubusercontent.com/36162708/124906039-4827f000-e004-11eb-81a5-75e26cdf3375.png">
</p>

### Precision
Precision is the percentage of relevant results, ie, it is the ratio of True Positives(TP) to all the positives in the dataset (actual results).
<p align="center">
  <img alt="Precision" src="https://user-images.githubusercontent.com/36162708/124906773-0fd4e180-e005-11eb-9af2-b2bb9b5a5f50.png">
</p>

### F1 Score
F1 is the weighted average of precision and recall of the model. It gives more importance to the false positives and false negatives while not letting large numbers of true negatives influence the score. A good F1 score is when there are low false positives and low false negatives. The F1 score ranges from 0 to 1 and is considered perfect when it's 1.
<p align="center">
  <img alt="F1 Score" src="https://user-images.githubusercontent.com/36162708/125073716-023d5b80-e0da-11eb-8b19-2b94e91c8cc2.png"
</p>


### BLEU Score
The BLEU (BiLingual Evaluation Understudy) score is a string-matching algorithm used for evaluating the quality of text which has been translated by a model from a language. The bleu metric ranges from 0 to 1 with 0 being the lowest score and 1 the highest. The closer the score is to 1, the more overlap there is with the reference translations. A higher score is also given to sequential matching words, ie, if a string of four words match the reference translation in the same exact order, it will have a more positive impact on the BLEU score than a string of two matching words.

### Perplexity
 Perplexity is defined as a measurement of how well a probability distribution or probability model predicts a sample. A better language model will have lower perplexity values or higher probability values for a test/valid set. It is also defined as exponential average negative log-likelihood of a sequence.<br>
  If we have a tokenized sequence X = (x0,x1,x2,...,xt) then the perplexity of X is:  
 <p align="center">
  <img alt="PPL" src="https://user-images.githubusercontent.com/36162708/125092264-ca8cde80-e0ee-11eb-9ddc-f8af1478bfa1.png">
</p>

### BERTScore
BertScore is a metric used for evaluating the text generated by a model. It computes a similarity score for each token in the predicted sentence with each token in the reference sentence using the contextual embeddings from the BERT model and generates scores in three common metrics- precision, recall and F1 measure.
  
## Model Metric Analysis:
  
We used the IMDB dataset for implementing Precision, Recall and F1 Score.

### Training Logs:
We trained the model for 25 Epochs, Below is the training logs for last 3 Epochs.
  Epoch: 23 | Epoch Time: 0m 8s

    +--------------------+-----------------+-----------------+
    |       Train        | Actual Positive | Actual Negative |
    +--------------------+-----------------+-----------------+
    | Predicted Positive |       8036      |       729       |
    | Predicted Negative |       654       |       8081      |
    +--------------------+-----------------+-----------------+
    +--------------------+-----------------+-----------------+
    |       Valid        | Actual Positive | Actual Negative |
    +--------------------+-----------------+-----------------+
    | Predicted Positive |       3111      |       460       |
    | Predicted Negative |       699       |       3230      |
    +--------------------+-----------------+-----------------+
    +-----------+--------+--------+
    |  Metrics  | Train  | Valid  |
    +-----------+--------+--------+
    |    Loss   | 0.212  | 0.397  |
    |  Accuracy | 92.09% | 84.52% |
    | Precision |  0.92  |  0.87  |
    |   Recall  |  0.92  |  0.82  |
    |  F1 score |  0.92  |  0.84  |
    +-----------+--------+--------+


    Epoch: 24 | Epoch Time: 0m 8s

    +--------------------+-----------------+-----------------+
    |       Train        | Actual Positive | Actual Negative |
    +--------------------+-----------------+-----------------+
    | Predicted Positive |       8078      |       637       |
    | Predicted Negative |       612       |       8173      |
    +--------------------+-----------------+-----------------+
    +--------------------+-----------------+-----------------+
    |       Valid        | Actual Positive | Actual Negative |
    +--------------------+-----------------+-----------------+
    | Predicted Positive |       3316      |       645       |
    | Predicted Negative |       494       |       3045      |
    +--------------------+-----------------+-----------------+
    +-----------+--------+--------+
    |  Metrics  | Train  | Valid  |
    +-----------+--------+--------+
    |    Loss   | 0.195  | 0.380  |
    |  Accuracy | 92.85% | 84.79% |
    | Precision |  0.93  |  0.84  |
    |   Recall  |  0.93  |  0.87  |
    |  F1 score |  0.93  |  0.85  |
    +-----------+--------+--------+


    Epoch: 25 | Epoch Time: 0m 8s

    +--------------------+-----------------+-----------------+
    |       Train        | Actual Positive | Actual Negative |
    +--------------------+-----------------+-----------------+
    | Predicted Positive |       8045      |       728       |
    | Predicted Negative |       645       |       8082      |
    +--------------------+-----------------+-----------------+
    +--------------------+-----------------+-----------------+
    |       Valid        | Actual Positive | Actual Negative |
    +--------------------+-----------------+-----------------+
    | Predicted Positive |       3357      |       723       |
    | Predicted Negative |       453       |       2967      |
    +--------------------+-----------------+-----------------+
    +-----------+--------+--------+
    |  Metrics  | Train  | Valid  |
    +-----------+--------+--------+
    |    Loss   | 0.208  | 0.381  |
    |  Accuracy | 92.16% | 84.26% |
    | Precision |  0.92  |  0.82  |
    |   Recall  |  0.93  |  0.88  |
    |  F1 score |  0.92  |  0.85  |
    +-----------+--------+--------+

A high precision rate of 0.82 signififes a low False Positive rate and a high recall of 0.88 suggests that the model has a low False Negative rate. Since both the precision and recall have high scores, we can conclude that the model is returning accurate results as well as returning a majority of the positive results.<br>
A F1 score of 0.85 signifies a low number of False positives and False negatives, that is, the model has a hgh recall and precision score. Hence the model is able to correctly label most of the positive samples successfully. 

We used the last model(German to English translation) for implementing Bleu score, PPL and Bert Score metrics.

### Training Logs:
We trained the model for 10 Epochs, Below is the training logs.
  
      Epoch: 01 | Time: 2m 13s
      Train Loss: 5.573 | Train PPL: 263.312
      Val. Loss: 5.219 |  Val. PPL: 184.807
      Bleu score:0.022
    Epoch: 02 | Time: 2m 15s
      Train Loss: 4.643 | Train PPL: 103.892
      Val. Loss: 4.984 |  Val. PPL: 146.114
      Bleu score:0.038
    Epoch: 03 | Time: 2m 14s
      Train Loss: 4.248 | Train PPL:  69.947
      Val. Loss: 4.861 |  Val. PPL: 129.106
      Bleu score:0.041
    Epoch: 04 | Time: 2m 14s
      Train Loss: 3.937 | Train PPL:  51.282
      Val. Loss: 4.581 |  Val. PPL:  97.588
      Bleu score:0.063
    Epoch: 05 | Time: 2m 15s
      Train Loss: 3.602 | Train PPL:  36.672
      Val. Loss: 4.349 |  Val. PPL:  77.398
      Bleu score:0.090
    Epoch: 06 | Time: 2m 15s
      Train Loss: 3.212 | Train PPL:  24.836
      Val. Loss: 4.139 |  Val. PPL:  62.758
      Bleu score:0.139
    Epoch: 07 | Time: 2m 14s
      Train Loss: 2.810 | Train PPL:  16.609
      Val. Loss: 3.910 |  Val. PPL:  49.902
      Bleu score:0.177
    Epoch: 08 | Time: 2m 14s
      Train Loss: 2.439 | Train PPL:  11.458
      Val. Loss: 3.846 |  Val. PPL:  46.796
      Bleu score:0.204
    Epoch: 09 | Time: 2m 14s
      Train Loss: 2.120 | Train PPL:   8.328
      Val. Loss: 3.799 |  Val. PPL:  44.674
      Bleu score:0.231
    Epoch: 10 | Time: 2m 16s
      Train Loss: 1.865 | Train PPL:   6.458
      Val. Loss: 3.803 |  Val. PPL:  44.836
      Bleu score:0.242
A bleu score of 23.405 means that the model has a low precision and there are several grammatical errors. This also indicates that the predicted sentences have less words overlapping with the reference sentences. This score was achieved by considering atmost 4-grams for the comparison process.<br>
  
The BERTScore can be evaluated in the same way we evaluate the precision, recall and F1 score metrics. Using BertScore, we have received a precision of 91.66 and a recall of 91.66. This means that the rate of False Positives and False negatives observed in the model is low and the model is predicting accurate results. A F1 score of 0.91 also suggests that the model is capable of correctly translating the sentences from German to English.
  

### Sample Outcomes:
  
    src = ['Ein', 'Mädchen', 'in', 'einem', 'Karateanzug', 'bricht', 'einen', 'Stock', 'mit', 'einem', 'Tritt', '.', '\n']
    trg = ['A', 'girl', 'in', 'karate', 'uniform', 'breaking', 'a', 'stick', 'with', 'a', 'front', 'kick', '.', '\n']
    predicted trg = ['A', 'girl', 'in', 'a', 'karate', 'uniform', 'is', 'a', 'stick', 'with', 'a', 'a', '.']  

<p align="center">
  <img alt="Attention!" src="https://user-images.githubusercontent.com/36162708/125081848-1e45fa80-e0e4-11eb-8c68-a61720a68467.png"
</p>
  
    src = ['Zwei', 'junge', 'weiße', 'Männer', 'sind', 'im', 'Freien', 'in', 'der', 'Nähe', 'vieler', 'Büsche', '.', '\n']
    trg = ['Two', 'young', ',', 'White', 'males', 'are', 'outside', 'near', 'many', 'bushes', '.', '\n']
    predicted trg = ['Two', 'young', 'white', 'men', 'are', 'outside', 'near', 'some', 'bushes', '.', '<eos>'] 
  
<p align="center">
  <img alt="Attention!" src="https://user-images.githubusercontent.com/36162708/125081964-4170aa00-e0e4-11eb-83ac-e83357abe4f9.png"
</p>
  
  
 ### Similarity matrix
  
    src = ['Ein', 'Mädchen', 'in', 'einem', 'Karateanzug', 'bricht', 'einen', 'Stock', 'mit', 'einem', 'Tritt', '.', '\n']
    trg = ['A', 'girl', 'in', 'karate', 'uniform', 'breaking', 'a', 'stick', 'with', 'a', 'front', 'kick', '.', '\n']
    predicted trg = ['A', 'girl', 'in', 'a', 'karate', 'uniform', 'is', 'a', 'stick', 'with', 'a', 'a', '.']  

<p align="center">
  <img alt="Attention!" src="https://user-images.githubusercontent.com/36162708/125081848-1e45fa80-e0e4-11eb-8c68-a61720a68467.png"
</p>
  
  
<p align="center">
  <img alt="Similarity Matrix!" src="https://user-images.githubusercontent.com/36162708/125081660-e17a0380-e0e3-11eb-8c3d-a4da12d34284.png"
</p>
  
