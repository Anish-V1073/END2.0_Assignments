{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment3_11.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H9c9SjGPR0j9",
        "outputId": "3272c2b0-d810-4ff4-861c-2751c050dfec"
      },
      "source": [
        "# Pytorch's tensors are similar to Numpy's ndarrays\n",
        "!pip install torch"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.8.1+cu101)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.7.4.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch) (1.19.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B0nxMQXlZuqT"
      },
      "source": [
        "import torch\n",
        "import torchvision # provide access to datasets, models, transforms, utils, etc\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UK0j5ODTaAyz"
      },
      "source": [
        "#Using MNIST Dataset for Training and Testing\n",
        "training_set = torchvision.datasets.MNIST(\n",
        "    root='./data',\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=transforms.Compose([transforms.ToTensor()])    \n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iYIZRg0iObYT"
      },
      "source": [
        "## Dataset Generation\n",
        "Creating a Custom dataset which consists of MNIST Image dataset, Random number and sum of the Image & a number.<br>\n",
        "By creating a custom class for dataset we can combine MNIST dataset, the random number(one-hot-encoded form) and sum of both."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1EZnGGYQBn9G"
      },
      "source": [
        "class MyTrainSet(Dataset):\n",
        "  def __init__(self, dataset):\n",
        "    #MNIST image dataset\n",
        "    self.ImageData = dataset\n",
        "    \n",
        "    #one-hot-encoding of digits 0-9\n",
        "    self.Integer = F.one_hot(torch.tensor([0,1,2,3,4,5,6,7,8,9]), num_classes=10)    \n",
        "\n",
        "  def __getitem__(self,index):\n",
        "    imagedata = self.ImageData[random.randint(0,59999)]\n",
        "    image,label = imagedata\n",
        "    RandNum = random.randint(0,9)\n",
        "    EncodedIp = self.Integer[RandNum] \n",
        "    #EncodedOp = self.Output[RandNum+label]   \n",
        "    return image,label,EncodedIp,(RandNum+label)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.ImageData)\n"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QH7NYnRBHVQS"
      },
      "source": [
        "Mydata = MyTrainSet(training_set)"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dxC8QIAEM2NT"
      },
      "source": [
        "## Data Visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ORZIH9Z8HeD3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "outputId": "a2d7a44a-5c3a-4054-e8be-5ae6f94a056d"
      },
      "source": [
        "Image, label, EncodedIp,Sum_2 = next(iter(Mydata))\n",
        "plt.imshow(Image.squeeze(), cmap='gray')\n",
        "print('label:', label)\n",
        "#print('Random Number:', RandNum)\n",
        "print('Encoded Input:', EncodedIp)\n",
        "#print('Encoded Output:', EncodedOp)\n",
        "print('Sum :', Sum_2)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "label: 5\n",
            "Encoded Input: tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0])\n",
            "Sum : 13\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJeyNE5hQD6f"
      },
      "source": [
        "## Building a Neural network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "csGGyOwvde7T"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Network(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        # Input Image 28x28 @1, Kernel_size = 5(i.e 5x5) \n",
        "        # therefore output image size= 24x24 @10(outchanels) and a maxpooling with Kernel=2 and stride =2\n",
        "        # The Output after Max-pooling will be 12x12@10\n",
        "        self.conv_layer1 = nn.Conv2d(in_channels=1, out_channels=10, kernel_size=5)\n",
        "\n",
        "        # Input Image 12x12 @10, Kernel_size = 5(i.e 5x5) \n",
        "        # therefore output image size= 8x8 @20(outchanels) and a maxpooling with Kernel=2 and stride =2\n",
        "        # The Output after Max-pooling will be 4x4@20\n",
        "        self.conv_layer2 = nn.Conv2d(in_channels=10, out_channels=20, kernel_size=5)\n",
        "        self.conv_dropout = nn.Dropout2d()\n",
        "\n",
        "\n",
        "        # Input Image 4x4@20, therefore 20*4*4 i/ps   \n",
        "        self.FCN1 = nn.Linear(in_features=320, out_features=50)\n",
        "\n",
        "        #Another layer of FCN with i/ps = 50 and o/ps = 10  (10 Classes)\n",
        "        self.out1 = nn.Linear(in_features=50, out_features=10)\n",
        "\n",
        "        # We are combining the output of the previous layer(with one-hot-encoded form)\n",
        "        self.FCN3 = nn.Linear(in_features=20, out_features=24)\n",
        "\n",
        "        # A Output layer with o/ps = 19 (the maximum sum of two digits will 18(i.e 19 including 0))\n",
        "        self.out2 = nn.Linear(in_features=24, out_features=19)\n",
        "\n",
        "        self.opfromimage = F.one_hot(torch.tensor([0,1,2,3,4,5,6,7,8,9]), num_classes=10)\n",
        "\n",
        "    def forward(self, inp1,inp2):\n",
        "        # (1) input layer\n",
        "        inp1 = inp1\n",
        "\n",
        "        # (2) hidden conv layer\n",
        "        N1 = self.conv_layer1(inp1)\n",
        "        N1 = F.max_pool2d(N1, kernel_size=2, stride=2)\n",
        "        N1 = F.relu(N1)\n",
        "        \n",
        "        # (3) hidden conv layer\n",
        "        N1 = self.conv_layer2(N1)\n",
        "        N1 = self.conv_dropout(N1)\n",
        "        N1 = F.max_pool2d(N1, kernel_size=2, stride=2)\n",
        "        N1 = F.relu(N1)        \n",
        "\n",
        "        # (4) hidden linear layer\n",
        "        N1 = N1.view(-1, 320)\n",
        "        N1 = self.FCN1(N1)\n",
        "        N1 = F.relu(N1)        \n",
        "\n",
        "        N1 = self.out1(N1)\n",
        "        N1 = F.log_softmax(N1,dim=1) \n",
        "        \n",
        "        # Generating i/p for nxt layer with previous layer o/ps\n",
        "        new_ip = self.opfromimage[N1.argmax(dim=1)]\n",
        "        \n",
        "        \n",
        "        Combined = torch.cat((new_ip, inp2), dim=1)\n",
        "        Combined = F.relu(Combined)\n",
        "\n",
        "        \n",
        "        # (6) hidden linear layer\n",
        "        Combined = self.FCN3(Combined)\n",
        "        Combined = F.relu(Combined)\n",
        "\n",
        "        # (7) hidden linear layer\n",
        "        Combined = self.out2(Combined)\n",
        "        Combined = F.softmax(Combined,dim=1)\n",
        "\n",
        "        return N1,Combined"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z-fPCwROUREo",
        "outputId": "e8d7544f-e748-4532-c0ae-5363c8ec97e6"
      },
      "source": [
        "network = Network()\n",
        "network"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Network(\n",
              "  (conv_layer1): Conv2d(1, 10, kernel_size=(5, 5), stride=(1, 1))\n",
              "  (conv_layer2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
              "  (conv_dropout): Dropout2d(p=0.5, inplace=False)\n",
              "  (FCN1): Linear(in_features=320, out_features=50, bias=True)\n",
              "  (out1): Linear(in_features=50, out_features=10, bias=True)\n",
              "  (FCN3): Linear(in_features=20, out_features=24, bias=True)\n",
              "  (out2): Linear(in_features=24, out_features=19, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "id": "aMUn51F7U01H",
        "outputId": "61dbd7a6-a63b-4084-b8ae-85d185f39b39"
      },
      "source": [
        "Sample = next(iter(Mydata))\n",
        "Image, label, EncodedIp,Sum_2 = Sample\n",
        "plt.imshow(Image.squeeze(), cmap='gray')\n",
        "print('label:', label)\n",
        "#print('Random Number:', RandNum)\n",
        "print('Encoded Input:', EncodedIp)\n",
        "#print('Encoded Output:', EncodedOp)\n",
        "print('Sum :', Sum_2)\n",
        "\n",
        "network = Network()\n",
        "EncodedIp = torch.tensor(EncodedIp,dtype=torch.float32)\n",
        "pred_image,sum = network(Image.unsqueeze(0),EncodedIp.unsqueeze(0))\n",
        "#OP1 = pred[0,:10]\n",
        "#OP2 = pred[0,10:]\n",
        "print('Predicted Image :',pred_image.argmax(dim=1))\n",
        "print('Predicted Sum :',sum.argmax(dim=1))\n"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "label: 0\n",
            "Encoded Input: tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 0])\n",
            "Sum : 6\n",
            "Predicted Image : tensor([0])\n",
            "Predicted Sum : tensor([6])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x2-GpnLpoY6S",
        "outputId": "eee45ec6-0773-4ea8-cc09-188811d654f7"
      },
      "source": [
        "data_loader = torch.utils.data.DataLoader(\n",
        "    Mydata, \n",
        "    batch_size=100\n",
        ")\n",
        "\n",
        "batch = next(iter(data_loader))\n",
        "len(batch)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KMPQkZLB1mmY"
      },
      "source": [
        "def get_num_correct(X, Y):\n",
        "  return X.argmax(dim=1).eq(Y).sum().item()"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p1DjHk6fsGib",
        "outputId": "0e2670da-076c-42d3-fe96-b957ac8350db"
      },
      "source": [
        "import torch.optim as optim\n",
        "torch.set_grad_enabled(True) # remember we turned off the gradients?"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch.autograd.grad_mode.set_grad_enabled at 0x7fcd21bedfd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "anESVvz7WDgv"
      },
      "source": [
        "optimizer = optim.Adam(network.parameters(), lr=0.01)"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L7MvSllrRzxq"
      },
      "source": [
        "## Tarining a Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6MDRI1c5OYy"
      },
      "source": [
        "train_loader = torch.utils.data.DataLoader(Mydata, batch_size=1000)\n",
        "optimizer = optim.Adam(network.parameters(), lr=0.05)\n",
        "\n",
        "for epoch in range(200):\n",
        "\n",
        "    total_loss_1 = 0\n",
        "    total_loss_2 = 0\n",
        "    total_correct = 0\n",
        "    total_correct_sum = 0\n",
        "\n",
        "    for batch in train_loader: # Get Batch\n",
        "        Image, label, EncodedIp,Sum_2 = batch\n",
        "        EncodedIp = torch.tensor(EncodedIp,dtype=torch.float32)\n",
        "        pred_image, Sum_3 = network(Image,EncodedIp)\n",
        "                \n",
        "        loss1 = F.cross_entropy(pred_image, label)\n",
        "        loss2 = F.cross_entropy(Sum_3, Sum_2)\n",
        "        Loss = loss1 + loss2\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        Loss.backward() # Calculate Gradients\n",
        "        #loss2.backward()\n",
        "        optimizer.step() # Update Weights        \n",
        "\n",
        "        total_loss_1 += Loss.item()\n",
        "        #total_loss_2 += loss2.item()\n",
        "               \n",
        "        total_correct += get_num_correct(pred_image, label)\n",
        "        total_correct_sum +=get_num_correct(Sum_3,Sum_2)\n",
        "        #print(\n",
        "        #    \"total_correct images:\", total_correct, \n",
        "        #    \"total_correct Sum:\", total_correct_sum\n",
        "        #)\n",
        "\n",
        "    ImageAccuracy = (total_correct/60000)*100;\n",
        "    SumAccuracy = (total_correct_sum/60000)*100;\n",
        "    print(\n",
        "        \"\\n\\nEpoch: \", epoch,\n",
        "        \"\\nBatch Size: \",1000,\n",
        "        \"\\nPredicted image Count: \", total_correct,\n",
        "        \"Predicted sum Count: \", total_correct_sum,\n",
        "        #\"loss_1:\", total_loss1,\n",
        "        \"\\nTotal loss:\", total_loss_1,\n",
        "        \"\\nImage Accuracy: \",ImageAccuracy,\"Sum Accuracy: \", SumAccuracy\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76eb1IYyLNKN"
      },
      "source": [
        "## Evaluating\n",
        "\n",
        "We are using same strategy used for generating the training dataset for Test dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h2LizMBuK2tt"
      },
      "source": [
        "test_set = torchvision.datasets.MNIST(\n",
        "    root='./data',\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=transforms.Compose([transforms.ToTensor()])    \n",
        ")"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V61qu4P4LQFs"
      },
      "source": [
        "class MyTestSet(Dataset):\n",
        "  def __init__(self, dataset):\n",
        "    #MNIST image dataset\n",
        "    self.ImageData = dataset\n",
        "    #one-hot-encoding of digits 0-9\n",
        "    self.Integer = F.one_hot(torch.tensor([0,1,2,3,4,5,6,7,8,9]), num_classes=10)\n",
        "    #self.Output = F.one_hot(torch.tensor([0,1,2,3,4,5,6,7,8,9,11,12,13,14,15,16,17,18]), num_classes=19)\n",
        "\n",
        "  def __getitem__(self,index):\n",
        "    imagedata = self.ImageData[random.randint(0,9999)]\n",
        "    image,label = imagedata\n",
        "    RandNum = random.randint(0,9)\n",
        "    EncodedIp = self.Integer[RandNum] \n",
        "    #EncodedOp = self.Output[RandNum+label]   \n",
        "    return image,label,EncodedIp,(RandNum+label)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.ImageData)"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vXYXReRZLZd3"
      },
      "source": [
        "MyTestData = MyTestSet(test_set)"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iM1iPzXQLk-1",
        "outputId": "3a0b2c8b-6be6-47f2-ec8b-179370c22f5c"
      },
      "source": [
        "test_loader = torch.utils.data.DataLoader(MyTestData, batch_size=100)\n",
        "total_correct = 0\n",
        "total_correct_sum = 0\n",
        "for batch in test_loader: # Get Batch\n",
        "  Image, label, EncodedIp,Sum_2 = batch\n",
        "  EncodedIp = torch.tensor(EncodedIp,dtype=torch.float32)\n",
        "  pred_image, Sum_3 = network(Image,EncodedIp)\n",
        "  total_correct += get_num_correct(pred_image, label)\n",
        "  total_correct_sum +=get_num_correct(Sum_3,Sum_2)\n",
        "  TestImageAccuracy = (total_correct/10000)*100;\n",
        "  TestSumAccuracy = (total_correct_sum/10000)*100;\n",
        "print(\n",
        "        \"\\nPredicted image Count: \", total_correct,\n",
        "        \"Predicted sum Count: \", total_correct_sum,        \n",
        "        \"\\nImage Accuracy: \",TestImageAccuracy,\"Sum Accuracy: \", TestSumAccuracy\n",
        ")"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Predicted image Count:  8976 Predicted sum Count:  7261 \n",
            "Image Accuracy:  89.75999999999999 Sum Accuracy:  72.61\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tWNqsBqpSQtq"
      },
      "source": [
        "## Results\n",
        "\n",
        "Training Set : 60000<br>\n",
        "Test Set : 10000\n",
        "<br><br>\n",
        "### Accuracy\n",
        "Image predicted correctly = 89%<br>\n",
        "Sum predicted correctly = 72%\n",
        "\n"
      ]
    }
  ]
}