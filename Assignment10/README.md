# Assignment 10

## Objective
* Replace the embeddings of this session's code with GloVe embeddings
* Compare your results with this session's code. 
* Upload to a public GitHub repo and proceed to Session 10 Assignment Solutions where these questions are asked: 
    * Share the link to your README file's public repo for this assignment. Expecting a minimum 500-word write-up on your learnings. Expecting you to compare your results with the code covered in the class. - 750 Points
    * Share the link to your main notebook with training logs - 250 Points


## GloVe
GloVe, stands for "Global Vectors", is a model for distributed word representation. The model is an unsupervised learning algorithm for obtaining vector representations for words. This is achieved by mapping words into a meaningful space where the distance between words is related to semantic similarity.<br>

## Dataset
The data consists of a set of thousands of English to French translation pairs. Each word in both the the languages will be represented as a one-hot vector. This process is handled by the Lang class. The data is normalized wherein it is transformed to lowercase and converted from unicode to ASCII. All non-letter characters are also omitted as part of the normalization process. Normalization is done to define the data in a standard form so as to reduce randomness and increase efficiency. Once the normalization process is completed, we reduce the size of the available input data using two parameters- the length of the sentence (maximum of 10 words) and certain prefixes found in the English language. At the end of this process, we have a standardised limited dataset of English to French pairs.

## Model
In this model, we attempt to perform language translation, specifically English to French, using a Sequence to Sequence network with Attention. For our Sequence to Sequence network, we consider two Recurrent Neural Networks (RNNs), one acting as the encoder and the other the decoder.  The encoder network takes an English sequence as input and and outputs a vector containing a value for each input word. The attention mechanism is implemented on this vector and it highlights relevant parts of the vector. With the help of the attention mechanism, we don't have to encode the full sentence into a fixed-length vector. We can let the decoder learn what to attend based on the input sentence produced so far at each step of the output generation. The decoder takes this weighted combination of all the input states from the attention mechanism and unravels it into a sequence of French words. Since the output of the model depends heavily on the vector generated by the encoder, teacher forcing can be used to guide the decoder so that the model is trained properly. In teacher forcing, the expected output from the training dataset at the current time step is used as input to the next time step instead of the output from the decoder at that time step.<br><br>
For building the model, we will be using GloVe (Global Vectors for Word Representation) embeddings in the embedding layer. GloVe embedding provide an added advantage in comparison to regular word embeddings wherein it takes into consideration not only the local context information of words but also the word's co-occurence in the full corpus. These embeddings relate to the probabilities that two words appear together. Hence, it adds some more meaning into word vectors by considering the relationships between word pair and word pair.<br>

In our model, we are using GloVe embeddings of 50 dimensions and adding 'SOS' and 'EOS' token to the start of the word embeddings. Once the word embeddings are ready and the data has been normalized, we build the weight matrix which is added to the embedding layer of the encoder. This weight matrix is built based on pre-trained weights of the glove vectors.

## Training Logs
      1m 55s (- 26m 58s) (5000 6%) 3.9277
      3m 48s (- 24m 45s) (10000 13%) 3.4028
      5m 42s (- 22m 49s) (15000 20%) 3.2060
      7m 35s (- 20m 53s) (20000 26%) 3.0135
      9m 29s (- 18m 59s) (25000 33%) 2.8964
      11m 23s (- 17m 5s) (30000 40%) 2.8528
      13m 18s (- 15m 12s) (35000 46%) 2.7711
      15m 12s (- 13m 18s) (40000 53%) 2.7209
      17m 6s (- 11m 24s) (45000 60%) 2.6759
      19m 1s (- 9m 30s) (50000 66%) 2.6174
      20m 55s (- 7m 36s) (55000 73%) 2.5449
      22m 49s (- 5m 42s) (60000 80%) 2.5144
      24m 42s (- 3m 48s) (65000 86%) 2.4566
      26m 37s (- 1m 54s) (70000 93%) 2.4286
      28m 30s (- 0m 0s) (75000 100%) 2.3680
  
 ### Sample Outcomes
      > i m employed by a french lawyer .
      = je suis employe par un avocat francais .
      < je suis un homme pour l un . <EOS>

      > i m pleased to see you .
      = je suis ravi de vous voir .
      < je suis heureux de vous voir . <EOS>

      > i m painting an easter egg .
      = je decore un uf de paques .
      < je suis un a un . <EOS>

      > you re such a cute boy .
      = tu es un garcon tellement mignon !
      < vous etes une qui un fort . <EOS>

      > they re still not safe .
      = ils ne sont toujours pas en securite .
      < ils ne sont pas encore . . <EOS>
      
#### With Attentions     
      input = she is older than me .
      output = elle est plus grand que lui . <EOS>
