# Assignment 10

## Objective
* Replace the embeddings of this session's code with GloVe embeddings
* Compare your results with this session's code. 
* Upload to a public GitHub repo and proceed to Session 10 Assignment Solutions where these questions are asked: 
    * Share the link to your README file's public repo for this assignment. Expecting a minimum 500-word write-up on your learnings. Expecting you to compare your results with the code covered in the class. - 750 Points
    * Share the link to your main notebook with training logs - 250 Points


## GloVe
GloVe, stands for "Global Vectors", is a model for distributed word representation. The model is an unsupervised learning algorithm for obtaining vector representations for words. This is achieved by mapping words into a meaningful space where the distance between words is related to semantic similarity.<br>

## Model
In this model, we attempt to perform language translation, specifically English to French, using a Sequence to Sequence network with Attention. For our Sequence to Sequence network, we consider two Recurrent Neural Networks (RNNs), one acting as the encoder and the other the decoder.  The encoder network takes an English sequence as input and and outputs a vector containing a value for each input word. The attention mechanism is implemented on this vector and it highlights relevant parts of the vector. With the help of the attention mechanism, we don't have to encode the full sentence into a fixed-length vector. We can let the decoder learn what to attend based on the input sentence produced so far at each step of the output generation. The decoder takes this weighted combination of all the input states from the attention mechanism and unravels it into a sequence of French words. Since the output of the model depends heavily on the vector generated by the encoder, teacher forcing can be used to guide the decoder so that the model is trained properly. In teacher forcing, the expected output from the training dataset at the current time step is used as input to the next time step instead of the output from the decoder at that time step.<br><br>
For building the model, we will be using GloVe (Global Vectors for Word Representation) embeddings in the embedding layer. GloVe embedding provide an added advantage in comparison to regular word embeddings wherein it takes into consideration not only the local context information of words but also the word's co-occurence in the full corpus. These embeddings relate to the probabilities that two words appear together. Hence, it adds some more meaning into word vectors by considering the relationships between word pair and word pair.<br>

In our model, we are using GloVe embeddings of 50 dimensions and adding 'SOS' and 'EOS' token to the start of the word embeddings. Once the word embeddings are ready and the data has been normalized, we build the weight matrix which is added to the embedding layer of the encoder. This weight matrix is built based on pre-trained weights of the glove vectors.

## Dataset
The data consists of a set of thousands of English to French translation pairs. Each word in both the the languages will be represented as a one-hot vector. This process is handled by the Lang class. The data is normalized wherein it is transformed to lowercase and converted from unicode to ASCII. All non-letter characters are also omitted as part of the normalization process. Normalization is done to define the data in a standard form so as to reduce randomness and increase efficiency. Once the normalization process is completed, we reduce the size of the available input data using two parameters- the length of the sentence (maximum of 10 words) and certain prefixes found in the English language. At the end of this process, we have a standardised limited dataset of English to French pairs.
